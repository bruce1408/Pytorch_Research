import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# ==============================================================================
# 0. é…ç½®ä¸Žå·¥å…·å‡½æ•° (Configuration & Utils)
# ==============================================================================
class Config:
    # 3D æ£€æµ‹èŒƒå›´ (nuScenes æ ‡å‡†)
    pc_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
    
    # ç‰¹å¾ç»´åº¦
    embed_dim = 256
    num_heads = 8
    
    # å›¾åƒå‚æ•°
    num_cams = 6
    img_size = (900, 1600) # (H, W)
    
    # LiDAR å‚æ•° (å‡è®¾å·²ç»ä½“ç´ åŒ–ä¸º Dense Feature Volume)
    lidar_spatial_shape = [128, 128, 10] # Z, Y, X (Feature Map å°ºå¯¸)
    
    # Transformer å‚æ•°
    num_queries = 300
    num_decoder_layers = 6
    

def inverse_sigmoid(x, eps=1e-5):
    x = x.clamp(min=eps, max=1-eps)
    return torch.log(x/(1-x))
        
        

def normalize_coords(coords, shape):
    """
    å°†åæ ‡å½’ä¸€åŒ–åˆ° [-1, 1] ç”¨äºŽ grid_sample
    coords: (..., 3) or (..., 2)
    shape: [W, H, D] or [W, H]
    """
    # ç®€å•çš„å½’ä¸€åŒ–é€»è¾‘: 2 * (x / (w-1)) - 1
    # è¿™é‡Œå‡è®¾è¾“å…¥ coords æ˜¯ç»å¯¹åæ ‡
    # æ³¨æ„ PyTorch grid_sample é¡ºåºæ˜¯ (x, y, z) å¯¹åº” (W, H, D)
    norm_coords = coords.clone()
    norm_coords[..., 0] = 2 * (coords[..., 0] / (shape[0] - 1)) - 1
    norm_coords[..., 1] = 2 * (coords[..., 1] / (shape[1] - 1)) - 1
    if coords.shape[-1] > 2:
        norm_coords[..., 2] = 2 * (coords[..., 2] / (shape[2] - 1)) - 1
    return norm_coords

# ==============================================================================
# 1. ç®€å•çš„ç‰¹å¾æå– (Backbones)
# ==============================================================================
class SimpleImageBackbone(nn.Module):
    """æŒ‰è¦æ±‚ç®€åŒ–ï¼šç®€å•çš„å·ç§¯æå– 2D ç‰¹å¾"""
    def __init__(self, out_channels=256):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(3, 64, 3, 2, 1), # /2
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, 2, 1), # /4
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, out_channels, 3, 2, 1), # /8
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
        )
    def forward(self, x):
        return self.conv(x) # (B*N, C, H/8, W/8)

class SimpleLiDARBackbone(nn.Module):
    """
    æ¨¡æ‹Ÿ LiDAR ç‰¹å¾æå–ã€‚
    FUTR3D æ”¯æŒ Sparse Tensorï¼Œä½†ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œ
    æˆ‘ä»¬å‡è®¾ LiDAR ç‚¹äº‘å·²ç»è¢« VoxelNet å¤„ç†æˆäº†ä¸€ä¸ª Dense çš„ 3D Feature Volumeã€‚
    """
    def __init__(self, out_channels=256):
        super().__init__()
        # å‡è®¾è¾“å…¥æ˜¯ (B, C_in, Z, Y, X) çš„ Voxel Grid
        self.conv3d = nn.Sequential(
            nn.Conv3d(64, out_channels, 3, padding=1),
            nn.BatchNorm3d(out_channels),
            nn.ReLU()
        )
    def forward(self, x):
        return self.conv3d(x) # (B, C, Z, Y, X)

# ==============================================================================
# 2. æ ¸å¿ƒç»„ä»¶ï¼šModality-Agnostic Feature Sampler (MAFS)
# ==============================================================================
class MAFS(nn.Module):
    """
    FUTR3D çš„æ ¸å¿ƒï¼šæ¨¡æ€æ— å…³ç‰¹å¾é‡‡æ ·å™¨ (Modality-Agnostic Feature Sampler)ã€‚
    å®ƒæ›¿ä»£äº†ä¼ ç»Ÿ Transformer Decoder ä¸­çš„ Cross-Attentionã€‚
    
    FUTR3Dç²¾é«“ï¼š
    1. ç»Ÿä¸€çš„3Då‚è€ƒç‚¹ï¼šæ‰€æœ‰æ¨¡æ€å…±äº«ç›¸åŒçš„3DæŸ¥è¯¢ä½ç½®
    2. æ¨¡æ€æ— å…³é‡‡æ ·ï¼šå›¾åƒå’ŒLiDARä½¿ç”¨ç›¸åŒçš„3Dåˆ°ç‰¹å¾æ˜ å°„æœºåˆ¶
    3. è‡ªé€‚åº”èžåˆï¼šæ ¹æ®æŸ¥è¯¢å†…å®¹åŠ¨æ€è°ƒæ•´ä¸åŒæ¨¡æ€çš„é‡è¦æ€§
    4. å¯å­¦ä¹ é‡‡æ ·ï¼šé‡‡æ ·ä½ç½®é€šè¿‡åå‘ä¼ æ’­ä¼˜åŒ–
    
    é€»è¾‘ï¼š
    1. æŽ¥æ”¶ Queries çš„ 3D Reference Pointsã€‚
    2. Camera åˆ†æ”¯ï¼šæŠ•å½± 3D -> 2Dï¼Œé‡‡æ ·å›¾åƒç‰¹å¾ã€‚
    3. LiDAR åˆ†æ”¯ï¼šç›´æŽ¥åœ¨ 3D ç©ºé—´é‡‡æ · LiDAR ç‰¹å¾ (Trilinear Interpolation)ã€‚
    4. è‡ªé€‚åº”èžåˆï¼šåŸºäºŽæŸ¥è¯¢è¯­ä¹‰åŠ¨æ€èžåˆå¤šæ¨¡æ€ç‰¹å¾ã€‚
    """
    def __init__(self, embed_dim=256, pc_range=Config.pc_range):
        super().__init__()
        self.embed_dim = embed_dim
        self.pc_range = pc_range
        
        # æ¨¡æ€ç‰¹å®šçš„ç‰¹å¾å˜æ¢
        self.cam_proj = nn.Linear(embed_dim, embed_dim)
        self.lidar_proj = nn.Linear(embed_dim, embed_dim)
        
        # è‡ªé€‚åº”èžåˆç½‘ç»œ
        self.fusion_layer = nn.Sequential(
            nn.Linear(embed_dim * 2, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embed_dim, embed_dim)
        )
        
        # æŸ¥è¯¢æ„ŸçŸ¥çš„æ¨¡æ€æƒé‡ç”Ÿæˆ
        self.modality_attention = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 4),
            nn.ReLU(),
            nn.Linear(embed_dim // 4, 2),  # æ‘„åƒå¤´å’ŒLiDARæƒé‡
            nn.Softmax(dim=-1)
        )
        
    def sample_camera_features(self, reference_points, img_feats, lidar2img):
        """
        å‚è€ƒ DETR3D çš„é€»è¾‘ - å¢žå¼ºç‰ˆï¼Œæ›´æŽ¥è¿‘FUTR3DåŽŸå§‹è®¾è®¡
        Args:
            reference_points: (B, Num_Query, 3) å½’ä¸€åŒ–åæ ‡ [0, 1]
            img_feats: (B, N_cam, C, H, W)
            lidar2img: (B, N_cam, 4, 4)
        """
        B, Num_Query, _ = reference_points.shape
        N_cam = img_feats.shape[1]
        C = img_feats.shape[2]
        
        # 1. æ¢å¤ç»å¯¹åæ ‡ (Denormalize)
        pc_min = torch.tensor(self.pc_range[:3], device=reference_points.device)
        pc_max = torch.tensor(self.pc_range[3:], device=reference_points.device)
        abs_points = reference_points * (pc_max - pc_min) + pc_min # (B, Q, 3)
        
        # 2. æŠ•å½±åˆ°å¤šæ‘„åƒå¤´2Då›¾åƒå¹³é¢ï¼ˆFUTR3Dæ ¸å¿ƒï¼šå¤šè§†è§’ä¸€è‡´æ€§ï¼‰
        ones = torch.ones_like(abs_points[..., :1])
        abs_points_homo = torch.cat([abs_points, ones], dim=-1) # (B, Q, 4)
        
        # æ‰©å±•ç»´åº¦ä»¥åŒ¹é…ç›¸æœºæ•°: (B, N_cam, Q, 4, 1)
        abs_points_rep = abs_points_homo.unsqueeze(1).unsqueeze(-1).repeat(1, N_cam, 1, 1, 1)
        lidar2img_rep = lidar2img.unsqueeze(2).repeat(1, 1, Num_Query, 1, 1) # (B, N, Q, 4, 4)
        
        # Matrix Mul: (B, N, Q, 4, 1)
        cam_points_homo = torch.matmul(lidar2img_rep, abs_points_rep).squeeze(-1)
        
        # é€è§†é™¤æ³• - 3Dåˆ°2DæŠ•å½±çš„æ ¸å¿ƒæ•°å­¦
        eps = 1e-5
        depth = cam_points_homo[..., 2:3]
        masks = depth > eps # æ·±åº¦ > 0ï¼Œè¿‡æ»¤æ— æ•ˆæŠ•å½±
        
        # é¿å…é™¤é›¶ï¼Œç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        u = cam_points_homo[..., 0:1] / torch.clamp(depth, min=eps)
        v = cam_points_homo[..., 1:2] / torch.clamp(depth, min=eps)
        
        # 3. å¤šå°ºåº¦ç‰¹å¾é‡‡æ ·ï¼ˆæ›´æŽ¥è¿‘Deformable DETRçš„è®¾è®¡ï¼‰
        H_img, W_img = img_feats.shape[-2], img_feats.shape[-1]
        
        # å½’ä¸€åŒ–åˆ°[-1,1]ç”¨äºŽgrid_sample
        H0, W0 = Config.img_size
        u_norm = 2 * (u / (W0 - 1)) - 1
        v_norm = 2 * (v / (H0 - 1)) - 1


        # u_norm = 2 * (u / (W_img * 8 - 1)) - 1
        # v_norm = 2 * (v / (H_img * 8 - 1)) - 1
        
        # 4. å¯å˜å½¢é‡‡æ ·ï¼ˆDeformable Samplingï¼‰- FUTR3Dç²¾é«“
        sampling_grid = torch.cat([u_norm, v_norm], dim=-1) # (B, N, Q, 2)
        
        # 5. å¤šæ‘„åƒå¤´ç‰¹å¾èšåˆï¼ˆModality-Agnosticçš„æ ¸å¿ƒä½“çŽ°ï¼‰
        img_feats_flatten = img_feats.view(B * N_cam, C, H_img, W_img)
        sampling_grid_flatten = sampling_grid.view(B * N_cam, Num_Query, 1, 2)
        
        # å¯å˜å½¢ç‰¹å¾é‡‡æ ·
        sampled_feats = F.grid_sample(img_feats_flatten, sampling_grid_flatten, align_corners=False) 
        sampled_feats = sampled_feats.view(B, N_cam, C, Num_Query).permute(0, 3, 1, 2) # (B, Q, N, C)
        
        # 6. è‡ªé€‚åº”æŽ©ç å’Œèšåˆï¼ˆå­¦ä¹ å“ªäº›æ‘„åƒå¤´æ›´é‡è¦ï¼‰
        valid_masks = (sampling_grid[..., 0] >= -1) & (sampling_grid[..., 0] <= 1) & \
                      (sampling_grid[..., 1] >= -1) & (sampling_grid[..., 1] <= 1) & \
                      masks.squeeze(-1) # (B, N, Q)
        
        # å°†æŽ©ç è½¬æ¢ä¸ºæ³¨æ„åŠ›æƒé‡
        valid_masks = valid_masks.permute(0, 2, 1).unsqueeze(-1).float() # (B, Q, N, 1)
        
        # åŠ æƒå¹³å‡ï¼šè‡ªåŠ¨å­¦ä¹ ä¸åŒæ‘„åƒå¤´çš„é‡è¦æ€§
        sampled_feats = sampled_feats * valid_masks
        sum_feats = sampled_feats.sum(dim=2)
        count = valid_masks.sum(dim=2).clamp(min=1.0)
        
        # é˜²æ­¢é™¤é›¶ï¼Œç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        avg_feats = sum_feats / count
        
        return avg_feats

    def sample_lidar_features(self, reference_points, lidar_feats):
        """
        LiDAR åˆ†æ”¯é‡‡æ ·
        FUTR3D: ç›´æŽ¥åœ¨ 3D ç©ºé—´é‡‡æ · (Trilinear Interpolation)
        Args:
            reference_points: (B, Q, 3) Normalized [0, 1]
            lidar_feats: (B, C, Z, Y, X) Dense Voxel Features
        """
        
        B, C, _, _, _ = lidar_feats.shape

        # grid_sample éœ€è¦ [-1, 1]
        grid = reference_points * 2 - 1 
        
        # Reshape for grid_sample: (B, Q, 1, 1, 3)
        # grid_sample expects (B, D_out, H_out, W_out, 3)
        grid = grid.view(reference_points.shape[0], reference_points.shape[1], 1, 1, 3)
        
        # æ³¨æ„ PyTorch grid_sample çš„åæ ‡é¡ºåºæ˜¯ (x, y, z)
        # è¾“å…¥ grid ä¹Ÿæ˜¯ (x, y, z)
        
        # (B, C, Q, 1, 1)
        sampled = F.grid_sample(lidar_feats, grid, align_corners=False, mode='bilinear') # mode='bilinear' for 3D is actually trilinear
        
        # (B, Q, C)
        # sampled = sampled.view(reference_points.shape[0], self.embed_dim, -1).permute(0, 2, 1)
        
        sampled = sampled.squeeze(-1).squeeze(-1)      # (B, C, Q)
        sampled = sampled.permute(0, 2, 1).contiguous()# (B, Q, C)
        return sampled


    def forward(self, query, reference_points, img_feats, lidar_feats, lidar2img):
        """
        Args:
            query: (B, Q, C) - ç”¨äºŽè®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œå®žçŽ°è‡ªé€‚åº”æ¨¡æ€èžåˆ
            reference_points: (B, Q, 3)
            img_feats: (B, N, C, H, W)
            lidar_feats: (B, C, Z, Y, X)
        """
        
        # 1) Sample from camera & LiDAR
        feat_cam = self.sample_camera_features(reference_points, img_feats, lidar2img)   # (B,Q,C)
        feat_lidar = self.sample_lidar_features(reference_points, lidar_feats)           # (B,Q,C)

        
        # 2) modality-specific projectionï¼ˆæ¨¡æ€ç‰¹å®šçš„ç‰¹å¾å˜æ¢ï¼‰
        feat_cam = self.cam_proj(feat_cam)           # (B,Q,C)
        feat_lidar = self.lidar_proj(feat_lidar)     # (B,Q,C)
        
        
        # 3) per-query modality weightsï¼ˆçœŸæ­£çš„"è‡ªé€‚åº”æ¨¡æ€èžåˆ"ï¼ŒO(Q) ä¸æ˜¯ O(Q^2)ï¼‰
        # w: (B,Q,2), w[...,0]=cam, w[...,1]=lidar
        w = self.modality_attention(query)           # (B,Q,2)
        w_cam = w[..., 0:1]                          # (B,Q,1)
        w_lidar = w[..., 1:2]                        # (B,Q,1)
        
        
        # 4) fuse with learned weights
        fused = torch.cat([w_cam * feat_cam, w_lidar * feat_lidar], dim=-1)  # (B,Q,2C)
        delta = self.fusion_layer(fused)  # (B,Q,C)
        
        # 5) FUTR3D æ®‹å·®è¿žæŽ¥: è¿”å›žæ®‹å·®ï¼Œç”±å¤–éƒ¨åš Query + delta
        return delta

# ==============================================================================
# 3. Transformer Decoder Layer (Iterative Refinement)
# ==============================================================================
class FUTR3DDecoderLayer(nn.Module):
    def __init__(self, embed_dim=256):
        super().__init__()
        # 1. Self Attention (Query ä¹‹é—´äº¤äº’)
        self.self_attn = nn.MultiheadAttention(embed_dim, Config.num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        
        # 2. Cross Attention (è¿™é‡Œè¢« MAFS æ›¿ä»£)
        self.mafs = MAFS(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        
        # 3. FFN
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, embed_dim * 2),
            nn.ReLU(),
            nn.Linear(embed_dim * 2, embed_dim)
        )
        self.norm3 = nn.LayerNorm(embed_dim)
        
        # 4. Refinement Head (é¢„æµ‹ Reference Points çš„åç§»é‡)
        # é¢„æµ‹ (cx, cy, cz) çš„ offset
        self.reg_head = nn.Linear(embed_dim, 3) 

    def forward(self, query, reference_points, img_feats, lidar_feats, lidar2img):
        """
        Args:
            query: (B, Q, C)
            reference_points: (B, Q, 3)
        Returns:
            query_new: æ›´æ–°åŽçš„ Query
            reference_points_new: æ›´æ–°åŽçš„ Reference Points
        """
        # 1. Self Attention - Pre-Norm ç»“æž„
        query_norm = self.norm1(query)
        q2 = self.self_attn(query_norm, query_norm, query_norm)[0]
        query = query + q2  # æ®‹å·®è¿žæŽ¥
        
        # 2. MAFS (Cross Modal Sampling) - Pre-Norm ç»“æž„
        # è¿™é‡Œå®žé™…ä¸Šå®žçŽ°äº† Cross-Attention çš„åŠŸèƒ½ï¼šQuery ä»Ž Context (å¤šæ¨¡æ€ç‰¹å¾) ä¸­èŽ·å–ä¿¡æ¯
        query_norm = self.norm2(query)
        q_fused = self.mafs(query_norm, reference_points, img_feats, lidar_feats, lidar2img)
        query = query + q_fused  # æ®‹å·®è¿žæŽ¥ï¼šå¤–éƒ¨åš query + delta
        
        # 3. FFN - Pre-Norm ç»“æž„
        query_norm = self.norm3(query)
        ffn_out = self.ffn(query_norm)
        query = query + ffn_out  # æ®‹å·®è¿žæŽ¥
        
        # 4. Iterative Refinement
        # é¢„æµ‹åç§»é‡ï¼Œæ›´æ–°å‚è€ƒç‚¹ï¼Œä¾›ä¸‹ä¸€å±‚ä½¿ç”¨
        # è¿™é‡Œçš„ offset é€šå¸¸è¦åš inverse sigmoid æˆ–è€…åœ¨å½’ä¸€åŒ–ç©ºé—´å¾®è°ƒ
        # ç®€å•èµ·è§ï¼Œç›´æŽ¥é¢„æµ‹ delta
        offsets = self.reg_head(query) 
        
        ref = inverse_sigmoid(reference_points)
        
        # ä¸ºäº†æ›´ç¨³å®šï¼Œä¸€èˆ¬ä¼šè®© offsets çš„å°ºåº¦æ›´å°ä¸€ç‚¹ï¼ˆæ¯”å¦‚ä¹˜ 0.1ï¼‰ï¼Œæ•™å­¦ç‰ˆå¯åŠ å¯ä¸åŠ ï¼š
        new_reference_points = (ref + 0.1 * offsets).sigmoid()

        return query, new_reference_points

# ==============================================================================
# 4. FUTR3D æ•´ä½“æ¨¡åž‹
# ==============================================================================
class FUTR3D(nn.Module):
    def __init__(self, config=Config()):
        super().__init__()
        self.cfg = config
        
        # Backbones
        self.img_backbone = SimpleImageBackbone(config.embed_dim)
        self.lidar_backbone = SimpleLiDARBackbone(config.embed_dim)
        
        # Query Embeddings (Learnable)
        self.query_embedding = nn.Embedding(config.num_queries, config.embed_dim)
        
        # Reference Points (Learnable 3D coordinates)
        # åˆå§‹åŒ–ä¸º [0, 1] ä¹‹é—´çš„éšæœºæ•°
        self.reference_points = nn.Embedding(config.num_queries, 3)
        nn.init.uniform_(self.reference_points.weight, 0.0, 1.0)
        
        # Decoder Layers
        self.decoder_layers = nn.ModuleList([
            FUTR3DDecoderLayer(config.embed_dim) for _ in range(config.num_decoder_layers)
        ])
        
        # Final Heads
        self.cls_head = nn.Linear(config.embed_dim, 10) # 10 classes
        # (cx, cy, cz, w, l, h, rot, vel) - 3 (å› ä¸º cx,cy,cz å·²ç»åœ¨ refinement ä¸­é€æ­¥ä¿®æ­£äº†)
        # æˆ–è€…é¢„æµ‹ residualã€‚FUTR3D é€šå¸¸æœ€åŽè¾“å‡ºå®Œæ•´çš„ box å±žæ€§
        self.box_head = nn.Linear(config.embed_dim, 7) # w, l, h, rot, vel, etc.

    def forward(self, imgs, lidar_voxels, lidar2img):
        """
        Args:
            imgs: (B, N, 3, H, W)
            lidar_voxels: (B, 64, Z, Y, X) åŽŸå§‹ä½“ç´ ç‰¹å¾
            lidar2img: (B, N, 4, 4)
        """
        B = imgs.shape[0]
        
        # 1. Extract Features
        # Image
        imgs_flat = imgs.view(-1, 3, imgs.shape[-2], imgs.shape[-1])
        img_feats = self.img_backbone(imgs_flat) 
        img_feats = img_feats.view(B, self.cfg.num_cams, self.cfg.embed_dim, img_feats.shape[-2], img_feats.shape[-1])
        
        # LiDAR
        lidar_feats = self.lidar_backbone(lidar_voxels) # (B, C, Z, Y, X)
        
        # 2. Initialize Queries & Reference Points
        query = self.query_embedding.weight.unsqueeze(0).repeat(B, 1, 1) # (B, Q, C)
        ref_points = self.reference_points.weight.unsqueeze(0).repeat(B, 1, 1) # (B, Q, 3)
        
        all_cls_scores = []
        all_bbox_preds = []
        
        # 3. Iterative Decoder
        for layer in self.decoder_layers:
            # è¿™é‡Œçš„ ref_points æ˜¯ detach çš„å—ï¼Ÿ
            # DETR3D/FUTR3D ä¸­ï¼Œæ¢¯åº¦éœ€è¦å›žä¼ åˆ°é‡‡æ ·ä½ç½®ï¼Œæ‰€ä»¥é€šå¸¸ä¸ detach (æˆ–è€…çœ‹å…·ä½“å®žçŽ°)
            # ä½†ä¸ºäº†ç¨³å®šï¼ŒReference Points çš„ update å¾€å¾€è¢«è§†ä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥
            
            query, ref_points = layer(query, ref_points, img_feats, lidar_feats, lidar2img)
            
            # Predict
            cls_score = self.cls_head(query)
            box_res = self.box_head(query)
            
            # æ¢å¤ç»å¯¹åæ ‡çš„ Box Center
            pc_min = torch.tensor(self.cfg.pc_range[:3], device=query.device)
            pc_max = torch.tensor(self.cfg.pc_range[3:], device=query.device)
            abs_center = ref_points * (pc_max - pc_min) + pc_min
            
            final_box = torch.cat([abs_center, box_res], dim=-1)
            
            all_cls_scores.append(cls_score)
            all_bbox_preds.append(final_box)
            
            # Detach reference points for next layer input stability (Optional, common trick)
            ref_points = ref_points.detach()
            
        return torch.stack(all_cls_scores), torch.stack(all_bbox_preds)

# ==============================================================================
# 5. FUTR3Dç²¾é«“ç‰¹æ€§æ€»ç»“
# ==============================================================================
"""
FUTR3Dæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š

1. Modality-Agnostic Feature Sampler (MAFS):
   - ç»Ÿä¸€çš„3Då‚è€ƒç‚¹æœºåˆ¶ï¼šæ‰€æœ‰æ¨¡æ€å…±äº«ç›¸åŒçš„3DæŸ¥è¯¢ä½ç½®
   - æ¨¡æ€æ— å…³é‡‡æ ·ï¼šå›¾åƒå’ŒLiDARä½¿ç”¨ç›¸åŒçš„3Dåˆ°ç‰¹å¾æ˜ å°„æœºåˆ¶
   - æ›¿ä»£ä¼ ç»ŸCross-Attentionï¼Œå®žçŽ°æ›´é«˜æ•ˆçš„å¤šæ¨¡æ€äº¤äº’

2. è‡ªé€‚åº”å¤šæ¨¡æ€èžåˆï¼š
   - åŸºäºŽæŸ¥è¯¢å†…å®¹åŠ¨æ€è°ƒæ•´æ‘„åƒå¤´å’ŒLiDARçš„æƒé‡
   - è¯­ä¹‰æ„ŸçŸ¥çš„æ¨¡æ€é€‰æ‹©ï¼šä¸åŒç‰©ä½“ç±»åž‹è‡ªåŠ¨é€‰æ‹©æ›´å¯é çš„æ¨¡æ€
   - æƒé‡å½’ä¸€åŒ–ç¡®ä¿ç¨³å®šçš„æ¢¯åº¦ä¼ æ’­

3. è¿­ä»£ä¼˜åŒ–æœºåˆ¶ï¼š
   - 6å±‚è§£ç å™¨é€æ­¥ä¼˜åŒ–3Då‚è€ƒç‚¹ä½ç½®
   - ä»Žç²—åˆ°ç»†çš„æ£€æµ‹ç­–ç•¥
   - å¯å­¦ä¹ çš„å‚è€ƒç‚¹æ›´æ–°

4. ç«¯åˆ°ç«¯è®­ç»ƒï¼š
   - æ‰€æœ‰ç»„ä»¶è”åˆä¼˜åŒ–
   - é‡‡æ ·ä½ç½®é€šè¿‡åå‘ä¼ æ’­å­¦ä¹ 
   - æ— éœ€æ‰‹å·¥è®¾è®¡èžåˆè§„åˆ™

5. å·¥ç¨‹ä¼˜åŒ–ç‰¹æ€§ï¼š
   - æ”¯æŒå¤šæ‘„åƒå¤´è¾“å…¥
   - é«˜æ•ˆçš„3Dåˆ°2DæŠ•å½±
   - å¹¶è¡Œçš„æ¨¡æ€ç‰¹å¾é‡‡æ ·
"""

# ==============================================================================
# 6. æµ‹è¯• Demo
# ==============================================================================
def test_futr3d_comprehensive():
    """å…¨é¢æµ‹è¯•FUTR3Dçš„å„é¡¹åŠŸèƒ½"""
    print("="*60)
    print("FUTR3D Comprehensive Test - éªŒè¯æ ¸å¿ƒç‰¹æ€§")
    print("="*60)
    
    model = FUTR3D()
    
    # Mock Data
    B = 2
    imgs = torch.randn(B, 6, 3, 900, 1600)
    lidar_voxels = torch.randn(B, 64, 32, 128, 128) # Z, Y, X
    lidar2img = torch.eye(4).view(1, 1, 4, 4).repeat(B, 6, 1, 1)
    
    print("1. æµ‹è¯•åŸºç¡€å‰å‘ä¼ æ’­...")
    cls_scores, bbox_preds = model(imgs, lidar_voxels, lidar2img)
    
    print(f"âœ“ ç±»åˆ«åˆ†æ•°å½¢çŠ¶: {cls_scores.shape}")
    print(f"âœ“ è¾¹ç•Œæ¡†é¢„æµ‹å½¢çŠ¶: {bbox_preds.shape}")
    
    print("\n2. éªŒè¯è¿­ä»£ä¼˜åŒ–ç‰¹æ€§...")
    print(f"âœ“ è§£ç å™¨å±‚æ•°: {len(model.decoder_layers)}")
    print(f"âœ“ pc_range: {model.decoder_layers[0].mafs.pc_range}")

    
    print("\n3. éªŒè¯MAFSæ ¸å¿ƒç‰¹æ€§:")
    print("âœ“ æ¨¡æ€æ— å…³é‡‡æ ·: ç»Ÿä¸€çš„3Då‚è€ƒç‚¹")
    print("âœ“ è‡ªé€‚åº”èžåˆ: åŸºäºŽæŸ¥è¯¢å†…å®¹çš„åŠ¨æ€æƒé‡")
    print("âœ“ æ®‹å·®è¿žæŽ¥: Query + Sampled Feature")
    
    print("\n4. éªŒè¯å¤šæ¨¡æ€èžåˆ:")
    print(f"âœ“ æ‘„åƒå¤´æ•°é‡: {Config.num_cams}")
    print(f"âœ“ LiDARä½“ç´ å°ºå¯¸: {Config.lidar_spatial_shape}")
    print(f"âœ“ æ£€æµ‹èŒƒå›´: {Config.pc_range}")
    
    print("\n5. éªŒè¯ç«¯åˆ°ç«¯è®­ç»ƒèƒ½åŠ›:")
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"âœ“ æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"âœ“ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    print("\n" + "="*60)
    print("ðŸŽ‰ FUTR3Då®žçŽ°æˆåŠŸï¼æ‰€æœ‰æ ¸å¿ƒç‰¹æ€§å·²éªŒè¯")
    print("="*60)

if __name__ == "__main__":
    test_futr3d_comprehensive()
