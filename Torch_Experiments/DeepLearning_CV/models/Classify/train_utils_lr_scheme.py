import os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt


def plot_lr_history(lr_history, title, output_dir="lr_plots"):
    """
    ä½¿ç”¨ matplotlib ç»˜åˆ¶å­¦ä¹ ç‡å˜åŒ–çš„å†å²è®°å½•ã€‚

    Args:
        lr_history (np.array): ä¸€ä¸ª numpy æ•°ç»„ï¼Œæ¯ä¸€è¡ŒåŒ…å« (è®­ç»ƒæ­¥æ•°, å­¦ä¹ ç‡)ã€‚
        title (str): å›¾è¡¨çš„æ ‡é¢˜ã€‚
    """
    
    # 1. ç¡®ä¿ä¿å­˜å›¾ç‰‡çš„æ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # 2. æ ¹æ®æ ‡é¢˜ç”Ÿæˆä¸€ä¸ªå®‰å…¨çš„æ–‡ä»¶å (æ›¿æ¢ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦)
    safe_filename = title.replace(" ", "_").replace("(", "").replace(")", "") + ".png"
    filepath = os.path.join(output_dir, safe_filename)


    # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢ï¼Œé¿å…åœ¨åŒä¸€å¼ å›¾ä¸Šé‡å¤ç»˜åˆ¶
    plt.figure(figsize=(10, 6))

    # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿ï¼Œå¹¶æ·»åŠ æ ‡è®°ç‚¹
    plt.plot(lr_history[:, 0], lr_history[:, 1], marker='o', linestyle='-')

    # --- ç¾åŒ–å›¾è¡¨ ---
    plt.title(title, fontsize=16)
    plt.xlabel("Training Step", fontsize=12)
    plt.ylabel("Learning Rate", fontsize=12)
    plt.grid(True)
    plt.tick_params(axis='both', labelsize=10)

    # æ˜¾ç¤ºå›¾è¡¨
    # plt.show()
    # 3. ä¿å­˜å›¾è¡¨åˆ°æ–‡ä»¶ï¼Œè€Œä¸æ˜¯æ˜¾ç¤ºå®ƒ
    plt.savefig(filepath)
    print(f"ğŸ“ˆ Plot saved to: {filepath}")

    # å…³é—­å½“å‰å›¾è¡¨ï¼Œä¸ºä¸‹ä¸€ä¸ªå›¾è¡¨åšå‡†å¤‡
    plt.close()


def run_scheduler_test(scheduler_name, scheduler_class, scheduler_params,
                       initial_lr=0.5, num_steps=20):
    """
    é€šç”¨çš„æµ‹è¯•å‡½æ•°ï¼Œç”¨äºæµ‹è¯•å„ç§ PyTorch å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
    å®ƒä¼šä¸ºæ¯ä¸ªæµ‹è¯•åˆ›å»ºç‹¬ç«‹çš„æ¨¡å‹å’Œä¼˜åŒ–å™¨ï¼Œä¿è¯æµ‹è¯•ç¯å¢ƒçš„çº¯å‡€ã€‚
    """
    print(f"\n--- Testing {scheduler_name} ---")

    # 1. ä¸ºæœ¬æ¬¡æµ‹è¯•åˆ›å»ºå…¨æ–°çš„æ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = nn.Conv2d(3, 64, 3)
    optimizer = optim.SGD(model.parameters(), lr=initial_lr)

    # 2. æ ¹æ®ä¼ å…¥çš„ç±»å’Œå‚æ•°ï¼Œå®ä¾‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = scheduler_class(optimizer, **scheduler_params)

    lr_history_list = []

    # 3. æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    for step in range(num_steps):
        # è®°å½•å½“å‰æ­¥çš„å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        lr_history_list.append((step, current_lr))
        print(f"Step {step}: LR = {current_lr:.5f}")

        # 4. æ¨¡æ‹Ÿä¸€æ¬¡è®­ç»ƒè¿­ä»£ (è¿™æ˜¯è°ƒç”¨ optimizer.step() çš„å‰æ)
        optimizer.zero_grad()
        # åˆ›å»ºä¸€ä¸ªå‡çš„è¾“å…¥å’Œè®¡ç®—ä¸€ä¸ªå‡çš„æŸå¤±
        dummy_input = torch.randn(1, 3, 64, 64)
        loss = model(dummy_input).sum()
        loss.backward()
        optimizer.step()

        # 5. æ›´æ–°å­¦ä¹ ç‡ (è¿™æ˜¯å­¦ä¹ ç‡è°ƒåº¦å™¨çš„æ ¸å¿ƒ)
        scheduler.step()

    lr_history = np.array(lr_history_list)
    plot_lr_history(lr_history, f"{scheduler_name} Learning Rate Schedule")


def test_warmup(init_lr=0.1, warmup_steps=5, total_steps=20):
    """
    æµ‹è¯•æ‰‹åŠ¨å®ç°çš„ Warmup å­¦ä¹ ç‡ç­–ç•¥ã€‚
    """
    print(f"\n--- Testing Manual Warmup ---")
    lr_history_list = []

    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    for step in range(total_steps):
        if step < warmup_steps:
            # åœ¨ Warmup é˜¶æ®µï¼Œå­¦ä¹ ç‡ä» 0 çº¿æ€§å¢åŠ åˆ° init_lr
            warmup_percent_done = (step + 1) / warmup_steps
            learning_rate = init_lr * warmup_percent_done
        else:
            # Warmup ç»“æŸåï¼Œä½¿ç”¨é¢„è®¾çš„å­¦ä¹ ç‡
            # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œé€šå¸¸ä¼šè¡”æ¥ä¸€ä¸ªå­¦ä¹ ç‡è¡°å‡ç­–ç•¥
            learning_rate = init_lr

        lr_history_list.append((step, learning_rate))
        print(f"Step {step}: LR = {learning_rate:.5f}")

    lr_history = np.array(lr_history_list)
    plot_lr_history(lr_history, f"Manual Warmup (for {warmup_steps} steps)")


# è„šæœ¬çš„ä¸»å…¥å£
if __name__ == '__main__':
    # --- Test 1: StepLR ---
    # æ¯éš” step_size æ­¥ï¼Œå°†å­¦ä¹ ç‡ä¹˜ä»¥ gamma
    run_scheduler_test(
        scheduler_name="StepLR",
        scheduler_class=optim.lr_scheduler.StepLR,
        scheduler_params={'step_size': 5, 'gamma': 0.5}
    )

    # --- Test 2: MultiStepLR ---
    # åœ¨ milestones æŒ‡å®šçš„æ­¥éª¤ï¼Œå°†å­¦ä¹ ç‡ä¹˜ä»¥ gamma
    run_scheduler_test(
        scheduler_name="MultiStepLR",
        scheduler_class=optim.lr_scheduler.MultiStepLR,
        scheduler_params={'milestones': [5, 10, 15], 'gamma': 0.5}
    )

    # --- Test 3: ExponentialLR ---
    # æ¯ä¸ªæ­¥éª¤éƒ½å°†å­¦ä¹ ç‡ä¹˜ä»¥ gammaï¼Œå®ç°æŒ‡æ•°è¡°å‡
    run_scheduler_test(
        scheduler_name="ExponentialLR",
        scheduler_class=optim.lr_scheduler.ExponentialLR,
        scheduler_params={'gamma': 0.85}
    )

    # --- Test 4: CosineAnnealingLR ---
    # å­¦ä¹ ç‡æŒ‰ç…§ä½™å¼¦æ›²çº¿å˜åŒ–ã€‚T_max æ˜¯åŠä¸ªå‘¨æœŸçš„æ­¥æ•°ã€‚
    # ä½™å¼¦é€€ç«é€šå¸¸åœ¨æ›´å¤šçš„æ­¥æ•°ä¸‹æ•ˆæœæ›´æ˜æ˜¾ï¼Œè¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿ50æ­¥ã€‚
    run_scheduler_test(
        scheduler_name="CosineAnnealingLR",
        scheduler_class=optim.lr_scheduler.CosineAnnealingLR,
        scheduler_params={'T_max': 50, 'eta_min': 0.01},
        num_steps=50
    )

    # --- Test 5: Warmup ---
    # è¿™æ˜¯æ‰‹åŠ¨å®ç°çš„ï¼Œæ‰€ä»¥å•ç‹¬è°ƒç”¨æµ‹è¯•å‡½æ•°
    test_warmup(init_lr=0.5, warmup_steps=5, total_steps=20)